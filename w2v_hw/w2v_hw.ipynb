{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "## 0.1 Task\n",
    "\n",
    "The tasks will be:\n",
    "1. Train a CBOW model with a real world dataset, explore how the parameters affect the model.\n",
    "2. Evaluate an embedding qualitatively\n",
    "\n",
    "## 0.2 Instruction\n",
    "\n",
    "To follow along this exercise, you want to execute the cells by clicking on them and press *Run* or hit *Shift+Enter*, from top to bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CBOW\n",
    "\n",
    "\n",
    "Our first task is to build our very own CBOW. In this section, you will **NOT** be required to write code for the network. Instead, you will be exploring the model by forming hypothesis and testing them through different parameter settings. Although you don't need to write code, you are encouraged to explore the model by writing code to test things out.\n",
    "\n",
    "First, let's remind ourselves of the famous quote:\n",
    "\n",
    "> You shall know a word by the company it keeps (Firth, J. R. 1957:11)\n",
    "\n",
    "What this implies is that it is possible to define a word, or *meaning* of a word in a way that describes a prediction task: **the task of predicting the word given the context**. However, one problem still remains: how to represent the meaning of a word? Luckily, there has already been a line of works that suggest a solution: representing the meaning of a word by a vector -- the vector space model.\n",
    "\n",
    "Now let's imagine that we are given a near-perfect vector space model that maps meaning of a word to vector. One of the easiest ways to solve the task then, is that we can literally just use the sum of the context vectors as the inputs, the target word vector as the output, and fit it through a linear model!\n",
    "\n",
    "And that is exactly what CBOW is doing: <img src=\"figures/cbow.png\" alt=\"cobw\" style=\"width: 400px;\"/>\n",
    "\n",
    "The weights between the input and hidden layer are the vector space model mapping, and the weights between the hidden and output layer are the linear prediction model. \n",
    "\n",
    "The only catch is that we don't have that near-perfect vector space model given to us! Thus, our goal is to jointly learn (1) a representation of the word, and (2) a prediction model. In the following sections, we will explore how to do that through PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 Building our training set\n",
    "\n",
    "We want to start with building a training set. For the purpose of this exploratory exercise, you will be using a medium size corpus: the [IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) (you should already downloaded it in `README.md`)\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "First, we removed all punctuations and lowercased everything, and tokenize it by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility collections for data processing, make sure you already download the data(see README.md)\n",
    "from utils import read_imdb_data\n",
    "\n",
    "# read_imdb_data removes punctuations and lowercase everything\n",
    "X_raw_train, y_train = read_imdb_data('../data/aclImdb/train')\n",
    "raw_text = ' '.join(X_raw_train).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that it is properly loaded, let's peek into the content a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was seriously looking forward to seeing this film because it seemed truly promising from the coming attractions jim carrey with godlike powers was an idea that most definitely worked\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(raw_text[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Vocab\n",
    "\n",
    "For the purpose of this assignment, and for the sake of training time, we limit the vocabulary to the most common 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# pick the top words only\n",
    "raw_text_count = Counter(raw_text)\n",
    "vocab = set(list(zip(*raw_text_count.most_common(1000)))[0])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then only keep the selected words in the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter raw text by vocab\n",
    "text = [r for r in raw_text if r in vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1\n",
    "\n",
    "The preprocessing steps introduced here seem very naive, and potentially problematic. Before you read further down the exercise, based on your understanding of CBOW, \n",
    "\n",
    "1. list three potential concerns with the preprocessing choices, explain why they might be a concern.\n",
    "2. potential fixes for these concerns.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataset\n",
    "\n",
    "Now let's build our dataset! First we define some handy helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    '''\n",
    "    helper function to translate context into indexes for inputs\n",
    "    In:\n",
    "        context: a list of words\n",
    "        word_to_ix: a mapping from word to index\n",
    "    Out:\n",
    "        idxs: a list of indexes\n",
    "    '''\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_batcher(X, Y, batch_size=50):\n",
    "    '''\n",
    "    helper function to batch data, batch_size is the size of \n",
    "    the batch.\n",
    "    In:\n",
    "        X: a matrix of size (num_sample, CONTEXT_SIZE*2)\n",
    "        Y: an array of size (num_sample)\n",
    "        batches: how many data points are in a batche, default:50\n",
    "    Out:\n",
    "        a batch of X and Y\n",
    "    '''\n",
    "    indices = np.arange(X.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    count = 0\n",
    "    \n",
    "    while count < X.shape[0]:\n",
    "        draw = indices[count:min(count+batch_size, X.shape[0])]\n",
    "        yield X[draw, :], Y[draw]\n",
    "        count += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our given (X) is the context, our target (Y) is the word. \n",
    "\n",
    "We then build our dataset by iterating through the filtered text and mapping the words to indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's set the context window size to 2 for now\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "\n",
    "# A mapping from word to index\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "# iterate through the text to build dataset\n",
    "for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE):\n",
    "    context = [text[i + j] for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1) if not j==0]\n",
    "    target = text[i]\n",
    "    # the data translate to indexes, X is the context, Y is the target word.\n",
    "    X.append(make_context_vector(context, word_to_ix))\n",
    "    Y.append(word_to_ix[target])\n",
    "\n",
    "# convert to numpy for easier data handling later\n",
    "X = np.array(X, dtype=int)\n",
    "Y = np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate what X and Y are a little bit by peeking into them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[745 553  38 821]\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first X, Y pair\n",
    "x0 = X[0]\n",
    "y0 = Y[0]\n",
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'seriously', 'looking', 'forward']\n",
      "[745, 553, 149, 38, 821]\n"
     ]
    }
   ],
   "source": [
    "# x0 is the indexes of the context, while y0 is the index of the target word.\n",
    "print(text[:5])\n",
    "print([word_to_ix[w] for w in text[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CBOW with PyTorch\n",
    "\n",
    "Now that we have our dataset built, let's import PyTorch!\n",
    "\n",
    "Note: if importing PyTorch failed, first try to click on **Kernel->Change Kernel->Python \\[conda env:pytorch_w2v\\]** on the upper bar of the notebook. It is likely that you did not set the kernel (which is what the jupyter notebook is running on) to the one you had pytorch installed. If you change the kernel, please re-run this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdf72ec53b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "# torch.nn contains modules or subcomponents of the network required to train the network.\n",
    "import torch.nn as nn\n",
    "# torch.nn.functional is a collection of handy functions that you can build into the model\n",
    "import torch.nn.functional as F\n",
    "# torch.optim contains optimizers to update the parameters of network\n",
    "import torch.optim as optim\n",
    "\n",
    "\"\"\"\n",
    "Variable in torch.autograd is used to tell pytorch that \n",
    "the object should be put into the PyTorch computation graph. \n",
    "See readings for more details.\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\"\"\"\n",
    "Let's also set a fix random seed so you can replicate the result.\n",
    "Note that neural network is sensitive to initial parameters. If\n",
    "you can't seem to produce a good result, you might want to use\n",
    "another random seed.\n",
    "\"\"\"\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at two modules of PyTorch that will be especially useful for our implementation of CBOW: `nn.Embedding` and `nn.Linear`\n",
    "\n",
    "Note: In PyTorch, if you don’t specify, the `Tensor` created (and thus the parameters in `nn.Embedding` and `nn.Linear`) will be all random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.Embedding.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to False, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
      "          additional dimensions\n",
      "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
      "          are the same shape as the input.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            (out_features x in_features)\n",
      "        bias:   the learnable bias of the module of shape (out_features)\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# print(nn.Linear.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CBOW model\n",
    "\n",
    "Now, it's the exciting part. Let's build our model!\n",
    "\n",
    "Recall that network structure of CBOW is equivalent to $A\\left ( \\sum_{w \\in Context} q_w \\right ) + b$, where $q_w$ is the vector representation of word $w$, A and b are parameters for the linear prediction model. If we applied log softmax to convert the output to log probability, the output of network then becomes: \n",
    "\n",
    "$$\\log P(w_i|context) = logSoftmax\\left (A\\left ( \\sum_{w \\in context} q_w \\right ) + b\\right )$$\n",
    "\n",
    "Our goal is then\n",
    "\n",
    "$$arg\\min_{A, b, Q} E_{(context, target) \\sim D} \\left [-\\log P(w_{target}|context)\\right ] $$\n",
    "\n",
    "where $Q$ is the matrix of embeddings, and D is a distribution that context and target words are drawn from.\n",
    "\n",
    "\n",
    "With this in mind, our model is thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of CBOW for exploratory purpose.\n",
    "    \n",
    "    Args:\n",
    "        - vocab_size: size of the vocabulary\n",
    "        - embedding_dim: dimension of the representation vector for words\n",
    "        - word_to_ix: a mapping from word to index\n",
    "    \n",
    "    Shape:\n",
    "        - Input: LongTensor (N, W), N = mini-batch size, \n",
    "                 W = number of indices to extract per mini-batch\n",
    "        - Output: (N, vocab_size),  N = mini-batch size\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing the model, instantiating the required module (Not linking them)\n",
    "    def __init__(self, vocab_size, embedding_dim, word_to_ix):\n",
    "        # A standard python way of saying CBOW is going to inherit nn.Module\n",
    "        super(CBOW, self).__init__()\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.emb = \n",
    "\n",
    "\n",
    "    # Here is where we acutally link the modules to describe how the data flow through the network.\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        - inputs: LongTensor (N, W), N = mini-batch size, \n",
    "                  W = number of indices to extract per mini-batch\n",
    "        - outputs: (N, vocab_size),  N = mini-batch size\n",
    "        \"\"\"\n",
    "        \n",
    "        return logsoftmax\n",
    "\n",
    "    # helper function to retrieve the trained vector space model (or word embedding)\n",
    "    def word_embedding(self):\n",
    "        return self.emb.weight\n",
    "    \n",
    "    # helper function to do a word to vector lookup\n",
    "    def word2vec(self, word):\n",
    "        return self.emb.weight[self.word_to_ix[word], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there really wasn't many lines of code! Most of those are comments!\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "Next we need to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handy library to help you visualize the progress\n",
    "import progressbar\n",
    "\n",
    "def train_cbow(model, num_epochs=3, batch_size=50):\n",
    "\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    \"\"\"\n",
    "    The stochastic gradient descent optimizer used to update weights, \n",
    "    once gradient is computed. We set a learning rate of 0.001, and \n",
    "    momentum of 0.9.\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # keep track of the loss of the model to see if the model converges.\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # progressbar setting\n",
    "        widgets = ['Epoch {} '.format(epoch), progressbar.Percentage(), ' ',\n",
    "                    progressbar.Bar(), ' ',\n",
    "                    progressbar.ETA()]\n",
    "        max_iteration = np.ceil(X.shape[0]/float(batch_size))\n",
    "        bar = progressbar.ProgressBar(widgets=widgets, max_value=max_iteration)\n",
    "\n",
    "        \"\"\"\n",
    "        bar(data_batcher(X, Y, batches)) is a way for progressbar to keep track of data_batcher(X, Y, batches)\n",
    "        bar(*) output whatever is pass into it, but also update the counter for progress bar\n",
    "        \"\"\"\n",
    "        for context, target in bar(data_batcher(X, Y, batch_size)):\n",
    "            \n",
    "            \"\"\"\n",
    "            Wrap our training pair context and target first through torch.LongTensor, \n",
    "            and then through Variable.\n",
    "            \n",
    "            Variable is used to put x and y into the PyTorch computation graph. See readings for more details.\n",
    "            \"\"\"\n",
    "            x = Variable(torch.LongTensor(context))\n",
    "            y = Variable(torch.LongTensor(target))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \"\"\"\n",
    "            forward\n",
    "            \n",
    "            The final piece how connecting the network to the data! \n",
    "            We feed the data in, and get a loss value back.\n",
    "            \"\"\"\n",
    "            \n",
    "            # run this input x forward through the network to get your output vector\n",
    "            outputs = model(x)\n",
    "            # compare outputs against correct output y and generate loss using the loss function\n",
    "            loss = loss_function(outputs, y)\n",
    "            \n",
    "            \"\"\"Backward\"\"\"\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Now that for every Variable, we have calculate the gradient, we can use an\n",
    "            optimizer to update the weights!\n",
    "            \n",
    "            SGD for example, does something intuitively like this:\n",
    "            w = w + learning_rate*w.grad\n",
    "            \"\"\"\n",
    "            optimizer.step()\n",
    "            \n",
    "            # just to record the loss\n",
    "            losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib to plot the losses\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initalize a CBOW model of vocab_size 1000 (the top 1000 words we limited it to) and train a vector space model with embbeding dimension=10, for the sake of shortening the training time. Note that a dimension=10 may not be optimal, and will be a parameter that you can potentially tune in Question 2.\n",
    "\n",
    "We then train the model for 3 epoches, with batch_size of 40 (there are 4404634 training samples). These are also parameters that you can potentially tune.\n",
    "\n",
    "This [link](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network) provides a nice discussion about the relationship between batch size and iterations. [Here](https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks) is a discussion for the definition of epoch, iteration, and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 100% |#################################################| Time:  0:06:22\n",
      "Epoch 1   1% |                                                 | ETA:   0:06:28"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1eb4530f9dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train with 3 epochs, batch size of 40. #100,000 data batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-e3b213dba4f3>\u001b[0m in \u001b[0;36mtrain_cbow\u001b[0;34m(model, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \"\"\"\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# run this input x forward through the network to get your output vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# compare outputs against correct output y and generate loss using the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/evann/data-ubuntu/perso/COURS DL/word2vec_assignment/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f500fb6096eb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         \u001b[0minput_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mhidden_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/evann/data-ubuntu/perso/COURS DL/word2vec_assignment/venv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/evann/data-ubuntu/perso/COURS DL/word2vec_assignment/venv/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/evann/data-ubuntu/perso/COURS DL/word2vec_assignment/venv/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CBOW(vocab_size=vocab_size, embedding_dim=10, word_to_ix=word_to_ix)\n",
    "\n",
    "# Train with 3 epochs, batch size of 40. #100,000 data batches\n",
    "plt.plot(train_cbow(model, num_epochs=3, batch_size=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation\n",
    "\n",
    "Now that we have a model trained, how do we know if it is good at all?\n",
    "\n",
    "**Sanity check**, or **smell test**, is a quick and dirty way to check if a model is doing anything reasonable at all. The idea of a sanity check is to check for a property that a good model should certainly hold. In our case, since we are training on a movie review dataset, we can safely assume that *'movie'* and *'film'* should have similar representation in a reasonably well-trained model, as they should appear in similar context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8891)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is PyTorch's cosine similarity module\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "# The word 'movie' and 'film' should appear in similar context, and thus should have similar representation.\n",
    "cos(model.word2vec('movie'), model.word2vec('film'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the 10 closest words to *movie*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_n_words(model, vocab, word, n=10):\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    word_vec = model.word2vec(word)\n",
    "    scores=[]\n",
    "    for v in vocab:\n",
    "        if not v == word:\n",
    "            score = cos(model.word2vec(v), word_vec)\n",
    "            scores.append((score, v))\n",
    "    \n",
    "    \"\"\"\n",
    "    return the n closest words to the target word\n",
    "\n",
    "    The purpose of using score[0].data.numpy() is to convert Tensor into\n",
    "    numpy array, since there are many things you cannot easily do to a\n",
    "    Tensor (like comparing two value and return a boolean, since it is \n",
    "    not differentiable!)\n",
    "    \"\"\"\n",
    "    n_closest = list(zip(*(sorted(scores, key=lambda score: score[0].data.numpy(), reverse=True)[:n])))[1]\n",
    "    return n_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('as',\n",
       " 'memorable',\n",
       " 'hand',\n",
       " 'director',\n",
       " 'comedy',\n",
       " 'life',\n",
       " 'filmed',\n",
       " 'usual',\n",
       " 'robert',\n",
       " 'meets')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_n_words(model, vocab, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
